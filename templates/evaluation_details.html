<h2>Dataset</h2>
        <p>
            To evaluate the performance of the proposed models in the challenge, we propose using a new Subtle-Diff evaluation dataset. This dataset was constructed using LLMs and image generation models to automatically generate similar image pairs containing subtle differences. Human annotators additionally described the differences in the generated image pairs with text. Subtle-Diff consists of 3000 image pairs (2100 for training, 600 for validation and 300 for testing) with 12,828 annotations for 570 different objects (see Table 1). Examples of images and captions from the Subtle-Diff dataset can be found in Figure 2.
        </p>
        
        <div class="table-container">
            <table>
                <caption>Table 1. Dataset Statistics</caption>
                <tr>
                    <th>Image pairs</th>
                    <th>Annotations</th>
                    <th>Objects</th>
                    <th>Annotators</th>
                    <th>Vocabulary</th>
                    <th>Sentence length</th>
                </tr>
                <tr>
                    <td>2848</td>
                    <td>12,828</td>
                    <td>570</td>
                    <td>11</td>
                    <td>1,930</td>
                    <td>12.78 (average)</td>
                </tr>
            </table>
        </div>

        <h2>Download Train and Val dataset splits</h2> 
        <p>You can download Train and Val dataset splits together with correspondent annotation sets from here: <a href="https://drive.google.com/file/d/1tGlHTOdL5stx2IhoF1B8qE872ct3X3RU/view?usp=sharing" target="_blank">here</a> </p>
        <p>Each dataset split has 2 annotations (for captioning and for classification tasks) and can be downloaded from here: <a href="https://drive.google.com/file/d/1tGlHTOdL5stx2IhoF1B8qE872ct3X3RU/view?usp=sharing" target="_blank">here</a></p>
        <p>The test split is not visible for the participants, to test your solution, you need to upload the predictions corresponding to ground truth using EvalAI.</p>

        <h2>Evaluation Metrics</h2>
        <p>
            For the difference image selection task, which is a binary classification task, we propose to use accuracy as the evaluation metric. 
        </p>
        <p>
            For the conditional difference captioning task, evaluation can be conducted using BLEU-4 and CIDEr metrics, widely used in image captioning tasks. Furthermore, the semantic alignment between predicted and correct texts can be assessed using GPT-4, facilitating the calculation of recall and precision. Recall is the proportion of all captions that semantically matched at least one of the ground truth texts. Precision is the proportion of annotations, excluding those described as having no difference, that were accurately matched to the image pairs.
        </p>
        
        <div class="image-container">
            <img src="figure2.png" alt="Figure 2: Examples of images and captions from the Subtle-Diff dataset">
            <p>Figure 2: Examples of images and captions from the Subtle-Diff dataset</p>
        </div>
